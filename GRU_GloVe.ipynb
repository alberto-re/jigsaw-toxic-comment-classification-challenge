{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/lib64/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals.joblib import dump\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Bidirectional, CuDNNGRU, SpatialDropout1D, PReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from common import SEED, TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 100000\n",
    "SEQ_MAX_LEN = 200\n",
    "EMBEDDINGS_FILE = \"glove.840B.300d.txt\"\n",
    "EMBEDDINGS_SIZE = 300\n",
    "WEIGHTS_CACHE = \"cache/gru_glove_weights.hdf5\"\n",
    "VALIDATION_PRED_FILE = \"cache/gru_glove_validation_pred_fold_%s.pkl\"\n",
    "SUBMISSION_FILE = \"submissions/submission_gru_glove.csv.gz\"\n",
    "K_FOLDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():  \n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_clean.csv\", encoding=\"utf-8\")\n",
    "test = pd.read_csv(\"data/test_clean.csv\", encoding=\"utf-8\")\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 s, sys: 76 ms, total: 12.2 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_tokenizer = text.Tokenizer(num_words=VOCABULARY_SIZE)\n",
    "word_tokenizer.fit_on_texts(train.comment_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.7 s, sys: 68 ms, total: 16.8 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_tokenized_word_train = word_tokenizer.texts_to_sequences(train.comment_text.values)\n",
    "list_tokenized_word_test = word_tokenizer.texts_to_sequences(test.comment_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.86 s, sys: 316 ms, total: 3.17 s\n",
      "Wall time: 3.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_word_sequences = sequence.pad_sequences(list_tokenized_word_train, maxlen=SEQ_MAX_LEN)\n",
    "x_test_word_sequences = sequence.pad_sequences(list_tokenized_word_test, maxlen=SEQ_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file):\n",
    "    embeddings = {}\n",
    "    f = open(file, 'r', encoding=\"utf-8\", errors=\"ignore\")\n",
    "    for index, line in enumerate(f):\n",
    "        try:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings[word] = coefs\n",
    "        except Exception:\n",
    "            print(\"Unable to parse line %d, skipping\" % index)\n",
    "    f.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to parse line 52343, skipping\n",
      "Unable to parse line 128261, skipping\n",
      "Unable to parse line 151102, skipping\n",
      "Unable to parse line 200668, skipping\n",
      "Unable to parse line 209833, skipping\n",
      "Unable to parse line 220779, skipping\n",
      "Unable to parse line 253461, skipping\n",
      "Unable to parse line 365745, skipping\n",
      "Unable to parse line 532048, skipping\n",
      "Unable to parse line 717302, skipping\n",
      "Unable to parse line 994818, skipping\n",
      "Unable to parse line 1123331, skipping\n",
      "Unable to parse line 1148409, skipping\n",
      "Unable to parse line 1352110, skipping\n",
      "Unable to parse line 1499727, skipping\n",
      "Unable to parse line 1533809, skipping\n",
      "Unable to parse line 1899841, skipping\n",
      "Unable to parse line 1921152, skipping\n",
      "Unable to parse line 2058966, skipping\n",
      "Unable to parse line 2165246, skipping\n",
      "Loaded 2195884 word vectors\n",
      "CPU times: user 2min 56s, sys: 4.58 s, total: 3min 1s\n",
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings = load_embeddings(EMBEDDINGS_FILE)\n",
    "print(\"Loaded %d word vectors\" % len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_matrix(embeddings, word_index):\n",
    "    embedding_matrix = np.zeros((VOCABULARY_SIZE, EMBEDDINGS_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= VOCABULARY_SIZE:\n",
    "            break\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = compute_embedding_matrix(embeddings, word_tokenizer.word_index)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(embedding_matrix):\n",
    "    nn = Sequential()\n",
    "    nn.add(Embedding(VOCABULARY_SIZE, EMBEDDINGS_SIZE, weights=[embedding_matrix], trainable=False, input_length=SEQ_MAX_LEN))\n",
    "    nn.add(SpatialDropout1D(0.2))\n",
    "    nn.add(Bidirectional(CuDNNGRU(128)))\n",
    "    nn.add(Dense(256))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(Dropout(0.5))\n",
    "    nn.add(Dense(6, activation=\"sigmoid\"))\n",
    "    nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=K_FOLDS, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "test_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/100\n",
      "143613/143613 [==============================] - 308s 2ms/step - loss: 0.0524 - acc: 0.9809 - val_loss: 0.0419 - val_acc: 0.9834\n",
      "Epoch 2/100\n",
      "143613/143613 [==============================] - 308s 2ms/step - loss: 0.0432 - acc: 0.9833 - val_loss: 0.0405 - val_acc: 0.9840\n",
      "Epoch 3/100\n",
      "143613/143613 [==============================] - 307s 2ms/step - loss: 0.0403 - acc: 0.9842 - val_loss: 0.0401 - val_acc: 0.9841\n",
      "Epoch 4/100\n",
      "143613/143613 [==============================] - 308s 2ms/step - loss: 0.0379 - acc: 0.9849 - val_loss: 0.0404 - val_acc: 0.9840\n",
      "Validation ROC-AUC score: 0.9877\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 307s 2ms/step - loss: 0.0526 - acc: 0.9808 - val_loss: 0.0440 - val_acc: 0.9828\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0432 - acc: 0.9834 - val_loss: 0.0414 - val_acc: 0.9833\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0403 - acc: 0.9841 - val_loss: 0.0433 - val_acc: 0.9831\n",
      "Validation ROC-AUC score: 0.9888\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0527 - acc: 0.9807 - val_loss: 0.0409 - val_acc: 0.9841\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0434 - acc: 0.9832 - val_loss: 0.0394 - val_acc: 0.9842\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0403 - acc: 0.9844 - val_loss: 0.0392 - val_acc: 0.9838\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0378 - acc: 0.9849 - val_loss: 0.0406 - val_acc: 0.9838\n",
      "Validation ROC-AUC score: 0.9905\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0522 - acc: 0.9809 - val_loss: 0.0483 - val_acc: 0.9820\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0430 - acc: 0.9835 - val_loss: 0.0430 - val_acc: 0.9838\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0404 - acc: 0.9842 - val_loss: 0.0418 - val_acc: 0.9842\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0377 - acc: 0.9849 - val_loss: 0.0447 - val_acc: 0.9840\n",
      "Validation ROC-AUC score: 0.9856\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 311s 2ms/step - loss: 0.0529 - acc: 0.9808 - val_loss: 0.0422 - val_acc: 0.9836\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0432 - acc: 0.9833 - val_loss: 0.0407 - val_acc: 0.9841\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0403 - acc: 0.9841 - val_loss: 0.0401 - val_acc: 0.9844\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0378 - acc: 0.9849 - val_loss: 0.0413 - val_acc: 0.9845\n",
      "Validation ROC-AUC score: 0.9875\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 310s 2ms/step - loss: 0.0527 - acc: 0.9807 - val_loss: 0.0412 - val_acc: 0.9844\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0434 - acc: 0.9832 - val_loss: 0.0393 - val_acc: 0.9846\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0405 - acc: 0.9840 - val_loss: 0.0396 - val_acc: 0.9843\n",
      "Validation ROC-AUC score: 0.9877\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 311s 2ms/step - loss: 0.0530 - acc: 0.9806 - val_loss: 0.0425 - val_acc: 0.9838\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 310s 2ms/step - loss: 0.0433 - acc: 0.9833 - val_loss: 0.0407 - val_acc: 0.9840\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 310s 2ms/step - loss: 0.0405 - acc: 0.9841 - val_loss: 0.0402 - val_acc: 0.9845\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 310s 2ms/step - loss: 0.0379 - acc: 0.9848 - val_loss: 0.0440 - val_acc: 0.9825\n",
      "Validation ROC-AUC score: 0.9877\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 310s 2ms/step - loss: 0.0523 - acc: 0.9810 - val_loss: 0.0436 - val_acc: 0.9831\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0431 - acc: 0.9835 - val_loss: 0.0418 - val_acc: 0.9835\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0403 - acc: 0.9843 - val_loss: 0.0410 - val_acc: 0.9838\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0378 - acc: 0.9850 - val_loss: 0.0423 - val_acc: 0.9835\n",
      "Validation ROC-AUC score: 0.9876\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 310s 2ms/step - loss: 0.0533 - acc: 0.9809 - val_loss: 0.0413 - val_acc: 0.9839\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0433 - acc: 0.9832 - val_loss: 0.0396 - val_acc: 0.9845\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0404 - acc: 0.9841 - val_loss: 0.0392 - val_acc: 0.9849\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0382 - acc: 0.9849 - val_loss: 0.0392 - val_acc: 0.9846\n",
      "Validation ROC-AUC score: 0.9883\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 311s 2ms/step - loss: 0.0524 - acc: 0.9808 - val_loss: 0.0429 - val_acc: 0.9828\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 310s 2ms/step - loss: 0.0432 - acc: 0.9834 - val_loss: 0.0423 - val_acc: 0.9836\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0403 - acc: 0.9842 - val_loss: 0.0408 - val_acc: 0.9841\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0379 - acc: 0.9849 - val_loss: 0.0427 - val_acc: 0.9835\n",
      "Validation ROC-AUC score: 0.9883\n"
     ]
    }
   ],
   "source": [
    "for index, (train_index, test_index) in enumerate(kfold.split(x_train_word_sequences, train[TARGETS].values)):\n",
    "    \n",
    "    x_train, x_val = x_train_word_sequences[train_index], x_train_word_sequences[test_index]\n",
    "    y_train, y_val = train[TARGETS].values[train_index], train[TARGETS].values[test_index]\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\"),\n",
    "        ModelCheckpoint(filepath=WEIGHTS_CACHE, save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    network = build_network(embedding_matrix)\n",
    "    history = network.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=100, batch_size=32, validation_data=(x_val, y_val), verbose=1, callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    network.load_weights(WEIGHTS_CACHE)\n",
    "    \n",
    "    y_pred = network.predict_proba(x_val)\n",
    "    dump(y_pred, VALIDATION_PRED_FILE % index)\n",
    "    \n",
    "    score = roc_auc_score(y_val, y_pred)\n",
    "    print(\"Validation ROC-AUC score: %0.4f\" % score)\n",
    "    scores.append(score)\n",
    "    \n",
    "    test_preds.append(network.predict_proba(x_test_word_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROC-AUC: 0.9880\n"
     ]
    }
   ],
   "source": [
    "print(\"Average ROC-AUC: %0.4f\" % np.mean(scores)) # 0.9868"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, target in enumerate(TARGETS):\n",
    "    y = 0\n",
    "    for fold in range(0, K_FOLDS):\n",
    "        y = y + test_preds[fold][:, index]\n",
    "    submission[target] = y / K_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(SUBMISSION_FILE, index=False, encoding=\"utf-8\", compression=\"gzip\") # Scores 0.9846"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
