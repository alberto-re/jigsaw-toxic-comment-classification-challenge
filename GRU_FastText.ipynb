{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/lib64/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals.joblib import dump\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Bidirectional, CuDNNGRU, SpatialDropout1D, PReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from common import SEED, TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 100000\n",
    "SEQ_MAX_LEN = 200\n",
    "EMBEDDINGS_FILE = \"crawl-300d-2M.vec\"\n",
    "EMBEDDINGS_SIZE = 300\n",
    "WEIGHTS_CACHE = \"cache/gru_fasttext_weights.hdf5\"\n",
    "VALIDATION_PRED_FILE = \"cache/gru_fasttext_validation_pred_fold_%s.pkl\"\n",
    "SUBMISSION_FILE = \"submissions/submission_gru_fasttext.csv.gz\"\n",
    "K_FOLDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_clean.csv\", encoding=\"utf-8\")\n",
    "test = pd.read_csv(\"data/test_clean.csv\", encoding=\"utf-8\")\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = text.Tokenizer(num_words=VOCABULARY_SIZE)\n",
    "word_tokenizer.fit_on_texts(train.comment_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_word_train = word_tokenizer.texts_to_sequences(train.comment_text.values)\n",
    "list_tokenized_word_test = word_tokenizer.texts_to_sequences(test.comment_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_word_sequences = sequence.pad_sequences(list_tokenized_word_train, maxlen=SEQ_MAX_LEN)\n",
    "x_test_word_sequences = sequence.pad_sequences(list_tokenized_word_test, maxlen=SEQ_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file):\n",
    "    embeddings = {}\n",
    "    f = open(file, 'r', encoding=\"utf-8\", errors=\"ignore\")\n",
    "    for index, line in enumerate(f):\n",
    "        try:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings[word] = coefs\n",
    "        except Exception:\n",
    "            print(\"Unable to parse line %d, skipping\" % index)\n",
    "    f.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 word vectors\n",
      "CPU times: user 2min 36s, sys: 4.25 s, total: 2min 40s\n",
      "Wall time: 4min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings = load_embeddings(EMBEDDINGS_FILE)\n",
    "print(\"Loaded %d word vectors\" % len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_matrix(embeddings, word_index):\n",
    "    embedding_matrix = np.zeros((VOCABULARY_SIZE, EMBEDDINGS_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= VOCABULARY_SIZE:\n",
    "            break\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = compute_embedding_matrix(embeddings, word_tokenizer.word_index)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(embedding_matrix):\n",
    "    nn = Sequential()\n",
    "    nn.add(Embedding(VOCABULARY_SIZE, EMBEDDINGS_SIZE, weights=[embedding_matrix], trainable=False, input_length=SEQ_MAX_LEN))\n",
    "    nn.add(SpatialDropout1D(0.2))\n",
    "    nn.add(Bidirectional(CuDNNGRU(128)))\n",
    "    nn.add(Dense(256))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(Dropout(0.5))\n",
    "    nn.add(Dense(6, activation=\"sigmoid\"))\n",
    "    nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=K_FOLDS, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "test_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/100\n",
      "143613/143613 [==============================] - 316s 2ms/step - loss: 0.0534 - acc: 0.9808 - val_loss: 0.0409 - val_acc: 0.9841\n",
      "Epoch 2/100\n",
      "143613/143613 [==============================] - 307s 2ms/step - loss: 0.0432 - acc: 0.9835 - val_loss: 0.0399 - val_acc: 0.9845\n",
      "Epoch 3/100\n",
      "143613/143613 [==============================] - 307s 2ms/step - loss: 0.0406 - acc: 0.9841 - val_loss: 0.0389 - val_acc: 0.9846\n",
      "Epoch 4/100\n",
      "143613/143613 [==============================] - 306s 2ms/step - loss: 0.0384 - acc: 0.9847 - val_loss: 0.0399 - val_acc: 0.9847\n",
      "Validation ROC-AUC score: 0.9894\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 307s 2ms/step - loss: 0.0545 - acc: 0.9806 - val_loss: 0.0438 - val_acc: 0.9828\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 307s 2ms/step - loss: 0.0437 - acc: 0.9832 - val_loss: 0.0417 - val_acc: 0.9837\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 307s 2ms/step - loss: 0.0410 - acc: 0.9840 - val_loss: 0.0433 - val_acc: 0.9834\n",
      "Validation ROC-AUC score: 0.9893\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 307s 2ms/step - loss: 0.0530 - acc: 0.9807 - val_loss: 0.0410 - val_acc: 0.9837\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 305s 2ms/step - loss: 0.0432 - acc: 0.9834 - val_loss: 0.0394 - val_acc: 0.9841\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 307s 2ms/step - loss: 0.0405 - acc: 0.9843 - val_loss: 0.0392 - val_acc: 0.9843\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 307s 2ms/step - loss: 0.0382 - acc: 0.9849 - val_loss: 0.0397 - val_acc: 0.9842\n",
      "Validation ROC-AUC score: 0.9903\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0528 - acc: 0.9809 - val_loss: 0.0468 - val_acc: 0.9828\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0430 - acc: 0.9833 - val_loss: 0.0421 - val_acc: 0.9845\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 305s 2ms/step - loss: 0.0403 - acc: 0.9842 - val_loss: 0.0410 - val_acc: 0.9845\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 307s 2ms/step - loss: 0.0380 - acc: 0.9849 - val_loss: 0.0427 - val_acc: 0.9843\n",
      "Validation ROC-AUC score: 0.9866\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0537 - acc: 0.9808 - val_loss: 0.0421 - val_acc: 0.9837\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0432 - acc: 0.9833 - val_loss: 0.0398 - val_acc: 0.9847\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 307s 2ms/step - loss: 0.0404 - acc: 0.9842 - val_loss: 0.0407 - val_acc: 0.9847\n",
      "Validation ROC-AUC score: 0.9874\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0533 - acc: 0.9807 - val_loss: 0.0403 - val_acc: 0.9844\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0433 - acc: 0.9833 - val_loss: 0.0402 - val_acc: 0.9847\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0406 - acc: 0.9840 - val_loss: 0.0382 - val_acc: 0.9852\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 307s 2ms/step - loss: 0.0380 - acc: 0.9848 - val_loss: 0.0388 - val_acc: 0.9848\n",
      "Validation ROC-AUC score: 0.9875\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 310s 2ms/step - loss: 0.0571 - acc: 0.9798 - val_loss: 0.0428 - val_acc: 0.9838\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0441 - acc: 0.9831 - val_loss: 0.0400 - val_acc: 0.9843\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0411 - acc: 0.9838 - val_loss: 0.0396 - val_acc: 0.9848\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0387 - acc: 0.9847 - val_loss: 0.0402 - val_acc: 0.9845\n",
      "Validation ROC-AUC score: 0.9883\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 311s 2ms/step - loss: 0.0531 - acc: 0.9810 - val_loss: 0.0449 - val_acc: 0.9835\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0433 - acc: 0.9832 - val_loss: 0.0408 - val_acc: 0.9837\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0403 - acc: 0.9842 - val_loss: 0.0406 - val_acc: 0.9837\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0382 - acc: 0.9848 - val_loss: 0.0414 - val_acc: 0.9840\n",
      "Validation ROC-AUC score: 0.9885\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 310s 2ms/step - loss: 0.0531 - acc: 0.9808 - val_loss: 0.0416 - val_acc: 0.9835\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0433 - acc: 0.9832 - val_loss: 0.0398 - val_acc: 0.9844\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 307s 2ms/step - loss: 0.0405 - acc: 0.9841 - val_loss: 0.0406 - val_acc: 0.9846\n",
      "Validation ROC-AUC score: 0.9883\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0531 - acc: 0.9807 - val_loss: 0.0461 - val_acc: 0.9825\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0429 - acc: 0.9834 - val_loss: 0.0431 - val_acc: 0.9832\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 309s 2ms/step - loss: 0.0402 - acc: 0.9843 - val_loss: 0.0408 - val_acc: 0.9841\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0379 - acc: 0.9850 - val_loss: 0.0402 - val_acc: 0.9842\n",
      "Epoch 5/100\n",
      "143614/143614 [==============================] - 308s 2ms/step - loss: 0.0359 - acc: 0.9855 - val_loss: 0.0419 - val_acc: 0.9840\n",
      "Validation ROC-AUC score: 0.9899\n"
     ]
    }
   ],
   "source": [
    "for index, (train_index, test_index) in enumerate(kfold.split(x_train_word_sequences, train[TARGETS].values)):\n",
    "    \n",
    "    x_train, x_val = x_train_word_sequences[train_index], x_train_word_sequences[test_index]\n",
    "    y_train, y_val = train[TARGETS].values[train_index], train[TARGETS].values[test_index]\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\"),\n",
    "        ModelCheckpoint(filepath=WEIGHTS_CACHE, save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    network = build_network(embedding_matrix)\n",
    "    history = network.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=100, batch_size=32, validation_data=(x_val, y_val), verbose=1, callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    network.load_weights(WEIGHTS_CACHE)\n",
    "    \n",
    "    y_pred = network.predict_proba(x_val)\n",
    "    dump(y_pred, VALIDATION_PRED_FILE % index)\n",
    "    \n",
    "    score = roc_auc_score(y_val, y_pred)\n",
    "    print(\"Validation ROC-AUC score: %0.4f\" % score)\n",
    "    scores.append(score)\n",
    "    \n",
    "    test_preds.append(network.predict_proba(x_test_word_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROC-AUC: 0.9886\n"
     ]
    }
   ],
   "source": [
    "print(\"Average ROC-AUC: %0.4f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, target in enumerate(TARGETS):\n",
    "    y = 0\n",
    "    for fold in range(0, K_FOLDS):\n",
    "        y = y + test_preds[fold][:, index]\n",
    "    submission[target] = y / K_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(SUBMISSION_FILE, index=False, encoding=\"utf-8\", compression=\"gzip\") # Scores 0.9847"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
