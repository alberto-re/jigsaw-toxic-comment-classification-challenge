{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alberto\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals.joblib import load, dump\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, SpatialDropout1D, Convolution1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from common import SEED, TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 100000\n",
    "SEQ_MAX_LEN = 100\n",
    "EMBEDDINGS_FILE = \"glove.840B.300d.txt\"\n",
    "EMBEDDINGS_SIZE = 300\n",
    "EMBEDDINGS_CACHE = \"cache/embeddings.pkl\"\n",
    "WEIGHTS_CACHE = \"cache/cnn_glove_weights.hdf5\"\n",
    "VALIDATION_PRED_FILE = \"cache/cnn_glove_validation_pred_fold_%s.pkl\"\n",
    "SUBMISSION_FILE = \"submissions/submission_cnn_glove.csv.gz\"\n",
    "K_FOLDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():  \n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_clean.csv\", encoding=\"utf-8\")\n",
    "test = pd.read_csv(\"data/test_clean.csv\", encoding=\"utf-8\")\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_tokenizer = text.Tokenizer(num_words=VOCABULARY_SIZE)\n",
    "word_tokenizer.fit_on_texts(train.comment_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_tokenized_word_train = word_tokenizer.texts_to_sequences(train.comment_text.values)\n",
    "list_tokenized_word_test = word_tokenizer.texts_to_sequences(test.comment_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_word_sequences = sequence.pad_sequences(list_tokenized_word_train, maxlen=SEQ_MAX_LEN)\n",
    "x_test_word_sequences = sequence.pad_sequences(list_tokenized_word_test, maxlen=SEQ_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    try:\n",
    "        return load(EMBEDDINGS_CACHE)\n",
    "    except IOError:\n",
    "        embeddings_index = {}\n",
    "        f = open(EMBEDDINGS_FILE, 'r', encoding=\"utf-8\", errors=\"ignore\")\n",
    "        for index, line in enumerate(f):\n",
    "            try:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "                embeddings_index[word] = coefs\n",
    "            except Exception:\n",
    "                print(\"Unable to parse line %d, skipping\" % index)\n",
    "        f.close()\n",
    "        dump(embeddings_index, EMBEDDINGS_CACHE)\n",
    "        return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2195884 word vectors\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings_index = load_embeddings()\n",
    "print(\"Loaded %s word vectors\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_matrix(embeddings_index, word_index):\n",
    "    embedding_matrix = np.zeros((VOCABULARY_SIZE, EMBEDDINGS_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= VOCABULARY_SIZE:\n",
    "            break\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 300)\n",
      "Wall time: 208 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedding_matrix = compute_embedding_matrix(embeddings_index, word_tokenizer.word_index)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(embedding_matrix):\n",
    "    nn = Sequential()\n",
    "    nn.add(Embedding(VOCABULARY_SIZE, EMBEDDINGS_SIZE, weights=[embedding_matrix], trainable=False, input_length=SEQ_MAX_LEN))\n",
    "    nn.add(SpatialDropout1D(0.3))\n",
    "    nn.add(Convolution1D(120, 3, padding=\"valid\", activation=\"relu\", strides=1))\n",
    "    nn.add(GlobalMaxPooling1D())\n",
    "    nn.add(Dense(120, activation=\"sigmoid\"))\n",
    "    nn.add(Dropout(0.5))\n",
    "    nn.add(Dense(6, activation=\"sigmoid\"))\n",
    "    nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=K_FOLDS, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "test_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/100\n",
      "143613/143613 [==============================] - 105s 730us/step - loss: 0.0619 - acc: 0.9784 - val_loss: 0.0514 - val_acc: 0.9797\n",
      "Epoch 2/100\n",
      "143613/143613 [==============================] - 96s 670us/step - loss: 0.0497 - acc: 0.9813 - val_loss: 0.0449 - val_acc: 0.9823\n",
      "Epoch 3/100\n",
      "143613/143613 [==============================] - 93s 648us/step - loss: 0.0474 - acc: 0.9820 - val_loss: 0.0462 - val_acc: 0.9815\n",
      "ROC-AUC score: 0.9841\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 95s 663us/step - loss: 0.0618 - acc: 0.9784 - val_loss: 0.0506 - val_acc: 0.9809\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 94s 653us/step - loss: 0.0495 - acc: 0.9815 - val_loss: 0.0484 - val_acc: 0.9808\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 93s 651us/step - loss: 0.0469 - acc: 0.9823 - val_loss: 0.0476 - val_acc: 0.9814\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 93s 647us/step - loss: 0.0454 - acc: 0.9828 - val_loss: 0.0459 - val_acc: 0.9825\n",
      "Epoch 5/100\n",
      "143614/143614 [==============================] - 92s 641us/step - loss: 0.0441 - acc: 0.9830 - val_loss: 0.0460 - val_acc: 0.9819\n",
      "ROC-AUC score: 0.9845\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 94s 654us/step - loss: 0.0640 - acc: 0.9776 - val_loss: 0.0477 - val_acc: 0.9816\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 93s 650us/step - loss: 0.0499 - acc: 0.9814 - val_loss: 0.0458 - val_acc: 0.9820\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 93s 647us/step - loss: 0.0472 - acc: 0.9820 - val_loss: 0.0450 - val_acc: 0.9827\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 93s 644us/step - loss: 0.0456 - acc: 0.9824 - val_loss: 0.0459 - val_acc: 0.9818\n",
      "ROC-AUC score: 0.9872\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 93s 650us/step - loss: 0.0611 - acc: 0.9782 - val_loss: 0.0506 - val_acc: 0.9808\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 92s 643us/step - loss: 0.0493 - acc: 0.9814 - val_loss: 0.0472 - val_acc: 0.9822\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 92s 643us/step - loss: 0.0468 - acc: 0.9820 - val_loss: 0.0479 - val_acc: 0.9823\n",
      "ROC-AUC score: 0.9800\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 94s 652us/step - loss: 0.0620 - acc: 0.9783 - val_loss: 0.0476 - val_acc: 0.9818\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 93s 647us/step - loss: 0.0497 - acc: 0.9814 - val_loss: 0.0451 - val_acc: 0.9826\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 92s 642us/step - loss: 0.0474 - acc: 0.9820 - val_loss: 0.0448 - val_acc: 0.9829\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 92s 641us/step - loss: 0.0457 - acc: 0.9824 - val_loss: 0.0453 - val_acc: 0.9828\n",
      "ROC-AUC score: 0.9840\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 93s 646us/step - loss: 0.0616 - acc: 0.9784 - val_loss: 0.0464 - val_acc: 0.9830\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 92s 643us/step - loss: 0.0492 - acc: 0.9814 - val_loss: 0.0453 - val_acc: 0.9832\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 92s 640us/step - loss: 0.0469 - acc: 0.9820 - val_loss: 0.0476 - val_acc: 0.9820\n",
      "ROC-AUC score: 0.9818\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 94s 654us/step - loss: 0.0617 - acc: 0.9784 - val_loss: 0.0472 - val_acc: 0.9823\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 92s 643us/step - loss: 0.0501 - acc: 0.9813 - val_loss: 0.0448 - val_acc: 0.9832\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 92s 641us/step - loss: 0.0473 - acc: 0.9820 - val_loss: 0.0449 - val_acc: 0.9828\n",
      "ROC-AUC score: 0.9819\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 93s 649us/step - loss: 0.0629 - acc: 0.9781 - val_loss: 0.0487 - val_acc: 0.9818\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 92s 638us/step - loss: 0.0497 - acc: 0.9815 - val_loss: 0.0463 - val_acc: 0.9823\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 92s 644us/step - loss: 0.0473 - acc: 0.9821 - val_loss: 0.0456 - val_acc: 0.9823\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 92s 640us/step - loss: 0.0453 - acc: 0.9826 - val_loss: 0.0458 - val_acc: 0.9824\n",
      "ROC-AUC score: 0.9837\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 93s 650us/step - loss: 0.0624 - acc: 0.9782 - val_loss: 0.0461 - val_acc: 0.9826\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 92s 640us/step - loss: 0.0497 - acc: 0.9814 - val_loss: 0.0446 - val_acc: 0.9827\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 92s 640us/step - loss: 0.0475 - acc: 0.9820 - val_loss: 0.0453 - val_acc: 0.9823\n",
      "ROC-AUC score: 0.9857\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 93s 649us/step - loss: 0.0623 - acc: 0.9783 - val_loss: 0.0483 - val_acc: 0.9816\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 92s 641us/step - loss: 0.0496 - acc: 0.9815 - val_loss: 0.0469 - val_acc: 0.9822\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 92s 638us/step - loss: 0.0474 - acc: 0.9821 - val_loss: 0.0462 - val_acc: 0.9828\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 92s 642us/step - loss: 0.0457 - acc: 0.9824 - val_loss: 0.0462 - val_acc: 0.9818\n",
      "Epoch 5/100\n",
      "143614/143614 [==============================] - 92s 638us/step - loss: 0.0443 - acc: 0.9828 - val_loss: 0.0464 - val_acc: 0.9824\n",
      "ROC-AUC score: 0.9852\n"
     ]
    }
   ],
   "source": [
    "for index, (train_index, test_index) in enumerate(kfold.split(x_train_word_sequences, train[TARGETS].values)):\n",
    "    \n",
    "    x_train, x_val = x_train_word_sequences[train_index], x_train_word_sequences[test_index]\n",
    "    y_train, y_val = train[TARGETS].values[train_index], train[TARGETS].values[test_index]\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\"),\n",
    "        ModelCheckpoint(filepath=WEIGHTS_CACHE, save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    network = build_network(embedding_matrix)\n",
    "    history = network.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=100, batch_size=32, validation_data=(x_val, y_val), verbose=1, callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    network.load_weights(WEIGHTS_CACHE)\n",
    "    \n",
    "    y_pred = network.predict_proba(x_val)\n",
    "    dump(y_pred, VALIDATION_PRED_FILE % index)\n",
    "    \n",
    "    score = roc_auc_score(y_val, y_pred)\n",
    "    print(\"ROC-AUC score: %0.4f\" % score)\n",
    "    scores.append(score)\n",
    "    \n",
    "    test_preds.append(network.predict_proba(x_test_word_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROC-AUC: 0.9838\n"
     ]
    }
   ],
   "source": [
    "print(\"Average ROC-AUC: %0.4f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, target in enumerate(TARGETS):\n",
    "    y = 0\n",
    "    for fold in range(0, K_FOLDS):\n",
    "        y = y + test_preds[fold][:, index]\n",
    "    submission[target] = y / K_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores 0.9807\n",
    "submission.to_csv(SUBMISSION_FILE, index=False, encoding=\"utf-8\", compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
