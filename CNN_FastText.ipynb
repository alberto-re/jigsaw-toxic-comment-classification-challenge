{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/lib64/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals.joblib import dump\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, SpatialDropout1D, Convolution1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from common import SEED, TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 100000\n",
    "SEQ_MAX_LEN = 100\n",
    "EMBEDDINGS_FILE = \"crawl-300d-2M.vec\"\n",
    "EMBEDDINGS_SIZE = 300\n",
    "WEIGHTS_CACHE = \"cache/cnn_fasttext_weights.hdf5\"\n",
    "VALIDATION_PRED_FILE = \"cache/cnn_fasttext_validation_pred_fold_%s.pkl\"\n",
    "SUBMISSION_FILE = \"submissions/submission_cnn_fasttext.csv.gz\"\n",
    "K_FOLDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():  \n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_clean.csv\", encoding=\"utf-8\")\n",
    "test = pd.read_csv(\"data/test_clean.csv\", encoding=\"utf-8\")\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 72 ms, total: 12 s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_tokenizer = text.Tokenizer(num_words=VOCABULARY_SIZE)\n",
    "word_tokenizer.fit_on_texts(train.comment_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.7 s, sys: 108 ms, total: 16.8 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_tokenized_word_train = word_tokenizer.texts_to_sequences(train.comment_text.values)\n",
    "list_tokenized_word_test = word_tokenizer.texts_to_sequences(test.comment_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.67 s, sys: 156 ms, total: 2.83 s\n",
      "Wall time: 2.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_word_sequences = sequence.pad_sequences(list_tokenized_word_train, maxlen=SEQ_MAX_LEN)\n",
    "x_test_word_sequences = sequence.pad_sequences(list_tokenized_word_test, maxlen=SEQ_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    embeddings = {}\n",
    "    f = open(EMBEDDINGS_FILE, 'r', encoding=\"utf-8\", errors=\"ignore\")\n",
    "    for index, line in enumerate(f):\n",
    "        try:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings[word] = coefs\n",
    "        except Exception:\n",
    "            print(\"Unable to parse line %d, skipping\" % index)\n",
    "    f.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings_index' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings = load_embeddings()\n",
    "print(\"Loaded %d word vectors\" % len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_matrix(embeddings, word_index):\n",
    "    embedding_matrix = np.zeros((VOCABULARY_SIZE, EMBEDDINGS_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= VOCABULARY_SIZE:\n",
    "            break\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 300)\n",
      "CPU times: user 240 ms, sys: 80 ms, total: 320 ms\n",
      "Wall time: 320 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedding_matrix = compute_embedding_matrix(embeddings, word_tokenizer.word_index)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(embedding_matrix):\n",
    "    nn = Sequential()\n",
    "    nn.add(Embedding(VOCABULARY_SIZE, EMBEDDINGS_SIZE, weights=[embedding_matrix], trainable=False, input_length=SEQ_MAX_LEN))\n",
    "    nn.add(SpatialDropout1D(0.3))\n",
    "    nn.add(Convolution1D(120, 3, padding=\"valid\", activation=\"relu\", strides=1))\n",
    "    nn.add(GlobalMaxPooling1D())\n",
    "    nn.add(Dense(120, activation=\"sigmoid\"))\n",
    "    nn.add(Dropout(0.5))\n",
    "    nn.add(Dense(6, activation=\"sigmoid\"))\n",
    "    nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=K_FOLDS, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "test_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/100\n",
      "143613/143613 [==============================] - 31s 219us/step - loss: 0.0612 - acc: 0.9785 - val_loss: 0.0465 - val_acc: 0.9816\n",
      "Epoch 2/100\n",
      "143613/143613 [==============================] - 31s 213us/step - loss: 0.0479 - acc: 0.9818 - val_loss: 0.0432 - val_acc: 0.9829\n",
      "Epoch 3/100\n",
      "143613/143613 [==============================] - 30s 211us/step - loss: 0.0451 - acc: 0.9827 - val_loss: 0.0460 - val_acc: 0.9816\n",
      "ROC-AUC score: 0.9856\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 31s 213us/step - loss: 0.0617 - acc: 0.9786 - val_loss: 0.0489 - val_acc: 0.9812\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 31s 213us/step - loss: 0.0474 - acc: 0.9821 - val_loss: 0.0462 - val_acc: 0.9819\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 30s 212us/step - loss: 0.0448 - acc: 0.9828 - val_loss: 0.0444 - val_acc: 0.9825\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 30s 212us/step - loss: 0.0432 - acc: 0.9834 - val_loss: 0.0441 - val_acc: 0.9828\n",
      "Epoch 5/100\n",
      "143614/143614 [==============================] - 30s 212us/step - loss: 0.0418 - acc: 0.9836 - val_loss: 0.0454 - val_acc: 0.9819\n",
      "ROC-AUC score: 0.9870\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 31s 214us/step - loss: 0.0637 - acc: 0.9777 - val_loss: 0.0462 - val_acc: 0.9819\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 30s 211us/step - loss: 0.0480 - acc: 0.9819 - val_loss: 0.0436 - val_acc: 0.9827\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 30s 212us/step - loss: 0.0448 - acc: 0.9829 - val_loss: 0.0427 - val_acc: 0.9833\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 30s 211us/step - loss: 0.0431 - acc: 0.9834 - val_loss: 0.0437 - val_acc: 0.9826\n",
      "ROC-AUC score: 0.9886\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 31s 215us/step - loss: 0.0602 - acc: 0.9786 - val_loss: 0.0487 - val_acc: 0.9814\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 31s 212us/step - loss: 0.0470 - acc: 0.9821 - val_loss: 0.0459 - val_acc: 0.9827\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 31s 215us/step - loss: 0.0447 - acc: 0.9828 - val_loss: 0.0453 - val_acc: 0.9830\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 31s 214us/step - loss: 0.0428 - acc: 0.9835 - val_loss: 0.0461 - val_acc: 0.9825\n",
      "ROC-AUC score: 0.9834\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 31s 218us/step - loss: 0.0602 - acc: 0.9788 - val_loss: 0.0445 - val_acc: 0.9827\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 31s 214us/step - loss: 0.0472 - acc: 0.9819 - val_loss: 0.0430 - val_acc: 0.9834\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 31s 214us/step - loss: 0.0446 - acc: 0.9828 - val_loss: 0.0430 - val_acc: 0.9835\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 31s 216us/step - loss: 0.0429 - acc: 0.9833 - val_loss: 0.0422 - val_acc: 0.9836\n",
      "Epoch 5/100\n",
      "143614/143614 [==============================] - 31s 214us/step - loss: 0.0418 - acc: 0.9836 - val_loss: 0.0430 - val_acc: 0.9837\n",
      "ROC-AUC score: 0.9851\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 31s 218us/step - loss: 0.0601 - acc: 0.9788 - val_loss: 0.0443 - val_acc: 0.9834\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 31s 215us/step - loss: 0.0472 - acc: 0.9820 - val_loss: 0.0436 - val_acc: 0.9834\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 31s 214us/step - loss: 0.0448 - acc: 0.9828 - val_loss: 0.0444 - val_acc: 0.9829\n",
      "ROC-AUC score: 0.9833\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 31s 217us/step - loss: 0.0588 - acc: 0.9791 - val_loss: 0.0459 - val_acc: 0.9822\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 31s 214us/step - loss: 0.0470 - acc: 0.9822 - val_loss: 0.0453 - val_acc: 0.9828\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 31s 214us/step - loss: 0.0445 - acc: 0.9829 - val_loss: 0.0448 - val_acc: 0.9828\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 31s 214us/step - loss: 0.0428 - acc: 0.9834 - val_loss: 0.0432 - val_acc: 0.9834\n",
      "Epoch 5/100\n",
      "143614/143614 [==============================] - 31s 214us/step - loss: 0.0416 - acc: 0.9837 - val_loss: 0.0432 - val_acc: 0.9835\n",
      "ROC-AUC score: 0.9851\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 31s 218us/step - loss: 0.0614 - acc: 0.9785 - val_loss: 0.0476 - val_acc: 0.9814\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 31s 216us/step - loss: 0.0475 - acc: 0.9819 - val_loss: 0.0445 - val_acc: 0.9828\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 31s 214us/step - loss: 0.0450 - acc: 0.9828 - val_loss: 0.0463 - val_acc: 0.9815\n",
      "ROC-AUC score: 0.9849\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 32s 220us/step - loss: 0.0619 - acc: 0.9785 - val_loss: 0.0442 - val_acc: 0.9827\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 31s 214us/step - loss: 0.0481 - acc: 0.9819 - val_loss: 0.0428 - val_acc: 0.9837\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 31s 215us/step - loss: 0.0452 - acc: 0.9828 - val_loss: 0.0420 - val_acc: 0.9836\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 31s 215us/step - loss: 0.0432 - acc: 0.9833 - val_loss: 0.0437 - val_acc: 0.9830\n",
      "ROC-AUC score: 0.9873\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 32s 224us/step - loss: 0.0596 - acc: 0.9790 - val_loss: 0.0471 - val_acc: 0.9820\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 31s 215us/step - loss: 0.0470 - acc: 0.9821 - val_loss: 0.0487 - val_acc: 0.9808\n",
      "ROC-AUC score: 0.9820\n"
     ]
    }
   ],
   "source": [
    "for index, (train_index, test_index) in enumerate(kfold.split(x_train_word_sequences, train[TARGETS].values)):\n",
    "    \n",
    "    x_train, x_val = x_train_word_sequences[train_index], x_train_word_sequences[test_index]\n",
    "    y_train, y_val = train[TARGETS].values[train_index], train[TARGETS].values[test_index]\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\"),\n",
    "        ModelCheckpoint(filepath=WEIGHTS_CACHE, save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    network = build_network(embedding_matrix)\n",
    "    history = network.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=100, batch_size=32, validation_data=(x_val, y_val), verbose=1, callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    network.load_weights(WEIGHTS_CACHE)\n",
    "    \n",
    "    y_pred = network.predict_proba(x_val)\n",
    "    dump(y_pred, VALIDATION_PRED_FILE % index)\n",
    "    \n",
    "    score = roc_auc_score(y_val, y_pred)\n",
    "    print(\"ROC-AUC score: %0.4f\" % score)\n",
    "    scores.append(score)\n",
    "    \n",
    "    test_preds.append(network.predict_proba(x_test_word_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROC-AUC: 0.9852\n"
     ]
    }
   ],
   "source": [
    "print(\"Average ROC-AUC: %0.4f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, target in enumerate(TARGETS):\n",
    "    y = 0\n",
    "    for fold in range(0, K_FOLDS):\n",
    "        y = y + test_preds[fold][:, index]\n",
    "    submission[target] = y / K_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(SUBMISSION_FILE, index=False, encoding=\"utf-8\", compression=\"gzip\")\n",
    "# Scores 0.9825"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
