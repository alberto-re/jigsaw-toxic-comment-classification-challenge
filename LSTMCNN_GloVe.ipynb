{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals.joblib import load, dump\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, SpatialDropout1D, Convolution1D, GlobalMaxPooling1D, CuDNNLSTM, PReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from common import SEED, TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 100000\n",
    "SEQ_MAX_LEN = 100\n",
    "EMBEDDINGS_FILE = \"glove.840B.300d.txt\"\n",
    "EMBEDDINGS_SIZE = 300\n",
    "EMBEDDINGS_CACHE = \"cache/embeddings.pkl\"\n",
    "WEIGHTS_CACHE = \"cache/lstm_cnn_glove_weights.hdf5\"\n",
    "VALIDATION_PRED_FILE = \"cache/lstm_cnn_glove_validation_pred_fold_%s.pkl\"\n",
    "SUBMISSION_FILE = \"submissions/submission_lstm_cnn_glove.csv.gz\"\n",
    "K_FOLDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():  \n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_clean.csv\", encoding=\"utf-8\")\n",
    "test = pd.read_csv(\"data/test_clean.csv\", encoding=\"utf-8\")\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 48 ms, total: 11.8 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_tokenizer = text.Tokenizer(num_words=VOCABULARY_SIZE)\n",
    "word_tokenizer.fit_on_texts(train.comment_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 s, sys: 128 ms, total: 16.6 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_tokenized_word_train = word_tokenizer.texts_to_sequences(train.comment_text.values)\n",
    "list_tokenized_word_test = word_tokenizer.texts_to_sequences(test.comment_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.6 s, sys: 132 ms, total: 2.73 s\n",
      "Wall time: 2.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_word_sequences = sequence.pad_sequences(list_tokenized_word_train, maxlen=SEQ_MAX_LEN)\n",
    "x_test_word_sequences = sequence.pad_sequences(list_tokenized_word_test, maxlen=SEQ_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    try:\n",
    "        return load(EMBEDDINGS_CACHE)\n",
    "    except IOError:\n",
    "        embeddings_index = {}\n",
    "        f = open(EMBEDDINGS_FILE, 'r', encoding=\"utf-8\", errors=\"ignore\")\n",
    "        for index, line in enumerate(f):\n",
    "            try:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "                embeddings_index[word] = coefs\n",
    "            except Exception:\n",
    "                print(\"Unable to parse line %d, skipping\" % index)\n",
    "        f.close()\n",
    "        dump(embeddings_index, EMBEDDINGS_CACHE)\n",
    "        return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2195884 word vectors\n",
      "CPU times: user 2min 27s, sys: 5.14 s, total: 2min 32s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings_index = load_embeddings()\n",
    "print(\"Loaded %s word vectors\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_matrix(embeddings_index, word_index):\n",
    "    embedding_matrix = np.zeros((VOCABULARY_SIZE, EMBEDDINGS_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= VOCABULARY_SIZE:\n",
    "            break\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 300)\n",
      "CPU times: user 232 ms, sys: 92 ms, total: 324 ms\n",
      "Wall time: 321 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedding_matrix = compute_embedding_matrix(embeddings_index, word_tokenizer.word_index)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(embedding_matrix):\n",
    "    nn = Sequential()\n",
    "    nn.add(Embedding(VOCABULARY_SIZE, EMBEDDINGS_SIZE, weights=[embedding_matrix], trainable=False, input_length=SEQ_MAX_LEN))\n",
    "    nn.add(SpatialDropout1D(0.2))\n",
    "    nn.add(CuDNNLSTM(SEQ_MAX_LEN, return_sequences=True))\n",
    "    nn.add(Convolution1D(256, 5, padding=\"valid\", strides=1))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(GlobalMaxPooling1D())\n",
    "    nn.add(Dense(256))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(Dropout(0.5))\n",
    "    nn.add(Dense(6, activation=\"sigmoid\"))\n",
    "    nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=K_FOLDS, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "test_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/100\n",
      "143613/143613 [==============================] - 104s 722us/step - loss: 0.0564 - acc: 0.9799 - val_loss: 0.0458 - val_acc: 0.9822\n",
      "Epoch 2/100\n",
      "143613/143613 [==============================] - 103s 718us/step - loss: 0.0461 - acc: 0.9827 - val_loss: 0.0416 - val_acc: 0.9838\n",
      "Epoch 3/100\n",
      "143613/143613 [==============================] - 103s 718us/step - loss: 0.0422 - acc: 0.9837 - val_loss: 0.0440 - val_acc: 0.9834\n",
      "ROC-AUC score: 0.9868\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 104s 725us/step - loss: 0.0562 - acc: 0.9800 - val_loss: 0.0483 - val_acc: 0.9817\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 104s 721us/step - loss: 0.0460 - acc: 0.9826 - val_loss: 0.0464 - val_acc: 0.9823\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 103s 719us/step - loss: 0.0423 - acc: 0.9836 - val_loss: 0.0449 - val_acc: 0.9826\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 104s 721us/step - loss: 0.0397 - acc: 0.9847 - val_loss: 0.0433 - val_acc: 0.9831\n",
      "Epoch 5/100\n",
      "143614/143614 [==============================] - 103s 720us/step - loss: 0.0373 - acc: 0.9853 - val_loss: 0.0437 - val_acc: 0.9833\n",
      "ROC-AUC score: 0.9873\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 104s 721us/step - loss: 0.0560 - acc: 0.9800 - val_loss: 0.0475 - val_acc: 0.9820\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 104s 722us/step - loss: 0.0461 - acc: 0.9827 - val_loss: 0.0418 - val_acc: 0.9834\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 103s 719us/step - loss: 0.0426 - acc: 0.9836 - val_loss: 0.0422 - val_acc: 0.9832\n",
      "ROC-AUC score: 0.9888\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 104s 725us/step - loss: 0.0559 - acc: 0.9799 - val_loss: 0.0481 - val_acc: 0.9822\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 104s 724us/step - loss: 0.0460 - acc: 0.9827 - val_loss: 0.0458 - val_acc: 0.9832\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 104s 722us/step - loss: 0.0423 - acc: 0.9838 - val_loss: 0.0439 - val_acc: 0.9838\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 104s 721us/step - loss: 0.0394 - acc: 0.9844 - val_loss: 0.0442 - val_acc: 0.9839\n",
      "ROC-AUC score: 0.9835\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 104s 726us/step - loss: 0.0564 - acc: 0.9799 - val_loss: 0.0443 - val_acc: 0.9828\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 103s 719us/step - loss: 0.0461 - acc: 0.9825 - val_loss: 0.0428 - val_acc: 0.9838\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 103s 719us/step - loss: 0.0425 - acc: 0.9838 - val_loss: 0.0420 - val_acc: 0.9842\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 104s 721us/step - loss: 0.0397 - acc: 0.9846 - val_loss: 0.0424 - val_acc: 0.9838\n",
      "ROC-AUC score: 0.9871\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 104s 724us/step - loss: 0.0561 - acc: 0.9798 - val_loss: 0.0457 - val_acc: 0.9827\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 104s 721us/step - loss: 0.0458 - acc: 0.9825 - val_loss: 0.0415 - val_acc: 0.9839\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 104s 721us/step - loss: 0.0422 - acc: 0.9837 - val_loss: 0.0419 - val_acc: 0.9837\n",
      "ROC-AUC score: 0.9834\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 105s 730us/step - loss: 0.0563 - acc: 0.9800 - val_loss: 0.0469 - val_acc: 0.9824\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 104s 724us/step - loss: 0.0462 - acc: 0.9826 - val_loss: 0.0472 - val_acc: 0.9821\n",
      "ROC-AUC score: 0.9800\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 105s 730us/step - loss: 0.0558 - acc: 0.9800 - val_loss: 0.0478 - val_acc: 0.9825\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 104s 723us/step - loss: 0.0457 - acc: 0.9827 - val_loss: 0.0436 - val_acc: 0.9829\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 104s 723us/step - loss: 0.0423 - acc: 0.9837 - val_loss: 0.0435 - val_acc: 0.9832\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 104s 722us/step - loss: 0.0397 - acc: 0.9844 - val_loss: 0.0439 - val_acc: 0.9828\n",
      "ROC-AUC score: 0.9850\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 105s 729us/step - loss: 0.0561 - acc: 0.9799 - val_loss: 0.0445 - val_acc: 0.9828\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 104s 723us/step - loss: 0.0463 - acc: 0.9826 - val_loss: 0.0422 - val_acc: 0.9840\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 104s 723us/step - loss: 0.0427 - acc: 0.9836 - val_loss: 0.0408 - val_acc: 0.9846\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 104s 726us/step - loss: 0.0400 - acc: 0.9844 - val_loss: 0.0417 - val_acc: 0.9845\n",
      "ROC-AUC score: 0.9874\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/100\n",
      "143614/143614 [==============================] - 106s 737us/step - loss: 0.0561 - acc: 0.9799 - val_loss: 0.0474 - val_acc: 0.9820\n",
      "Epoch 2/100\n",
      "143614/143614 [==============================] - 104s 728us/step - loss: 0.0459 - acc: 0.9827 - val_loss: 0.0456 - val_acc: 0.9826\n",
      "Epoch 3/100\n",
      "143614/143614 [==============================] - 104s 724us/step - loss: 0.0422 - acc: 0.9837 - val_loss: 0.0432 - val_acc: 0.9839\n",
      "Epoch 4/100\n",
      "143614/143614 [==============================] - 104s 727us/step - loss: 0.0397 - acc: 0.9843 - val_loss: 0.0433 - val_acc: 0.9837\n",
      "ROC-AUC score: 0.9879\n"
     ]
    }
   ],
   "source": [
    "for index, (train_index, test_index) in enumerate(kfold.split(x_train_word_sequences, train[TARGETS].values)):\n",
    "    \n",
    "    x_train, x_val = x_train_word_sequences[train_index], x_train_word_sequences[test_index]\n",
    "    y_train, y_val = train[TARGETS].values[train_index], train[TARGETS].values[test_index]\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\"),\n",
    "        ModelCheckpoint(filepath=WEIGHTS_CACHE, save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    network = build_network(embedding_matrix)\n",
    "    history = network.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=100, batch_size=32, validation_data=(x_val, y_val), verbose=1, callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    network.load_weights(WEIGHTS_CACHE)\n",
    "    \n",
    "    y_pred = network.predict_proba(x_val)\n",
    "    dump(y_pred, VALIDATION_PRED_FILE % index)\n",
    "    \n",
    "    score = roc_auc_score(y_val, y_pred)\n",
    "    print(\"ROC-AUC score: %0.4f\" % score)\n",
    "    scores.append(score)\n",
    "    \n",
    "    test_preds.append(network.predict_proba(x_test_word_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROC-AUC: 0.9857\n"
     ]
    }
   ],
   "source": [
    "print(\"Average ROC-AUC: %0.4f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, target in enumerate(TARGETS):\n",
    "    y = 0\n",
    "    for fold in range(0, K_FOLDS):\n",
    "        y = y + test_preds[fold][:, index]\n",
    "    submission[target] = y / K_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(SUBMISSION_FILE, index=False, encoding=\"utf-8\", compression=\"gzip\")\n",
    "# Scores 0.9841"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
